<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-10T00:09:33-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">AI4</title><subtitle>AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.</subtitle><author><name>AI4 member(s)</name></author><entry><title type="html">Weak Supervision for Real World Graphs</title><link href="http://localhost:4000/publications/2506.02451/" rel="alternate" type="text/html" title="Weak Supervision for Real World Graphs" /><published>2025-06-03T00:00:00-04:00</published><updated>2025-06-03T00:00:00-04:00</updated><id>http://localhost:4000/publications/2506.02451</id><content type="html" xml:base="http://localhost:4000/publications/2506.02451/"><![CDATA[<p><em>Pratheeksha Nair, Reihaneh Rabbany</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2506.02451" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Node classification in real world graphs often suffers from label scarcity and noise, especially in high stakes domains like human trafficking detection and misinformation monitoring. While direct supervision is limited, such graphs frequently contain weak signals, noisy or indirect cues, that can still inform learning. We propose WSNET, a novel weakly supervised graph contrastive learning framework that leverages these weak signals to guide robust representation learning. WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective tailored for weakly labeled data. Across three real world datasets and synthetic benchmarks with controlled noise, WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score. Our results highlight the effectiveness of contrastive learning under weak supervision and the promise of exploiting imperfect labels in graph based settings.</p>]]></content><author><name>Pratheeksha Nair</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Pratheeksha Nair, Reihaneh Rabbany Paper Abstract Node classification in real world graphs often suffers from label scarcity and noise, especially in high stakes domains like human trafficking detection and misinformation monitoring. While direct supervision is limited, such graphs frequently contain weak signals, noisy or indirect cues, that can still inform learning. We propose WSNET, a novel weakly supervised graph contrastive learning framework that leverages these weak signals to guide robust representation learning. WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective tailored for weakly labeled data. Across three real world datasets and synthetic benchmarks with controlled noise, WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score. Our results highlight the effectiveness of contrastive learning under weak supervision and the promise of exploiting imperfect labels in graph based settings.]]></summary></entry><entry><title type="html">It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics</title><link href="http://localhost:4000/publications/2506.02873/" rel="alternate" type="text/html" title="It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics" /><published>2025-06-03T00:00:00-04:00</published><updated>2025-06-03T00:00:00-04:00</updated><id>http://localhost:4000/publications/2506.02873</id><content type="html" xml:base="http://localhost:4000/publications/2506.02873/"><![CDATA[<p><em>Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2506.02873" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders’’ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model’s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval</p>]]></content><author><name>Kellin Pelrine</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine Paper Abstract Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders’’ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model’s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval]]></summary></entry><entry><title type="html">Are Large Language Models Good Temporal Graph Learners?</title><link href="http://localhost:4000/publications/2506.05393/" rel="alternate" type="text/html" title="Are Large Language Models Good Temporal Graph Learners?" /><published>2025-06-03T00:00:00-04:00</published><updated>2025-06-03T00:00:00-04:00</updated><id>http://localhost:4000/publications/2506.05393</id><content type="html" xml:base="http://localhost:4000/publications/2506.05393/"><![CDATA[<p><em>Shenyang Huang, Alipanah Parviz, Emma Kondrup, Zachary Yang, Zifeng Ding, Michael Bronstein, Reihaneh Rabbany, Guillaume Rabusseau</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2506.05393" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Large Language Models (LLMs) have recently driven significant advancements in Natural Language Processing and various other applications. While a broad range of literature has explored the graph-reasoning capabilities of LLMs, including their use of predictors on graphs, the application of LLMs to dynamic graphs – real world evolving networks – remains relatively unexplored. Recent work studies synthetic temporal graphs generated by random graph models, but applying LLMs to real-world temporal graphs remains an open question. To address this gap, we introduce Temporal Graph Talker (TGTalker), a novel temporal graph learning framework designed for LLMs. TGTalker utilizes the recency bias in temporal graphs to extract relevant structural information, converted to natural language for LLMs, while leveraging temporal neighbors as additional information for prediction. TGTalker demonstrates competitive link prediction capabilities compared to existing Temporal Graph Neural Network (TGNN) models. Across five real-world networks, TGTalker performs competitively with state-of-the-art temporal graph methods while consistently outperforming popular models such as TGN and HTGN. Furthermore, TGTalker generates textual explanations for each prediction, thus opening up exciting new directions in explainability and interpretability for temporal link prediction. The code is publicly available at https://github.com/shenyangHuang/TGTalker.</p>]]></content><author><name>Shenyang Huang</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Shenyang Huang, Alipanah Parviz, Emma Kondrup, Zachary Yang, Zifeng Ding, Michael Bronstein, Reihaneh Rabbany, Guillaume Rabusseau Paper Abstract Large Language Models (LLMs) have recently driven significant advancements in Natural Language Processing and various other applications. While a broad range of literature has explored the graph-reasoning capabilities of LLMs, including their use of predictors on graphs, the application of LLMs to dynamic graphs – real world evolving networks – remains relatively unexplored. Recent work studies synthetic temporal graphs generated by random graph models, but applying LLMs to real-world temporal graphs remains an open question. To address this gap, we introduce Temporal Graph Talker (TGTalker), a novel temporal graph learning framework designed for LLMs. TGTalker utilizes the recency bias in temporal graphs to extract relevant structural information, converted to natural language for LLMs, while leveraging temporal neighbors as additional information for prediction. TGTalker demonstrates competitive link prediction capabilities compared to existing Temporal Graph Neural Network (TGNN) models. Across five real-world networks, TGTalker performs competitively with state-of-the-art temporal graph methods while consistently outperforming popular models such as TGN and HTGN. Furthermore, TGTalker generates textual explanations for each prediction, thus opening up exciting new directions in explainability and interpretability for temporal link prediction. The code is publicly available at https://github.com/shenyangHuang/TGTalker.]]></summary></entry><entry><title type="html">Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection</title><link href="http://localhost:4000/publications/2506.06347/" rel="alternate" type="text/html" title="Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection" /><published>2025-06-01T00:00:00-04:00</published><updated>2025-06-01T00:00:00-04:00</updated><id>http://localhost:4000/publications/2506.06347</id><content type="html" xml:base="http://localhost:4000/publications/2506.06347/"><![CDATA[<p><em>Zachary Yang, Domenico Tullo, Reihaneh Rabbany</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2506.06347" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.</p>]]></content><author><name>Zachary Yang</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Zachary Yang, Domenico Tullo, Reihaneh Rabbany Paper Abstract Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.]]></summary></entry><entry><title type="html">Rendering-Aware Reinforcement Learning for Vector Graphics Generation</title><link href="http://localhost:4000/publications/2505.20793/" rel="alternate" type="text/html" title="Rendering-Aware Reinforcement Learning for Vector Graphics Generation" /><published>2025-05-27T00:00:00-04:00</published><updated>2025-05-27T00:00:00-04:00</updated><id>http://localhost:4000/publications/2505.20793</id><content type="html" xml:base="http://localhost:4000/publications/2505.20793/"><![CDATA[<p><em>Juan A. Rodriguez, Haotian Zhang, Abhay Puri, Aarash Feizi, Rishav Pramanik, Pascal Wichmann, Arnab Mondal, Mohammad Reza Samsami, Rabiul Awal, Perouz Taslakian, Spandana Gella, Sai Rajeswar, David Vázquez, Christopher Pal, Marco Pedersoli</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2505.20793" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.</p>]]></content><author><name>Aarash Feizi</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Juan A. Rodriguez, Haotian Zhang, Abhay Puri, Aarash Feizi, Rishav Pramanik, Pascal Wichmann, Arnab Mondal, Mohammad Reza Samsami, Rabiul Awal, Perouz Taslakian, Spandana Gella, Sai Rajeswar, David Vázquez, Christopher Pal, Marco Pedersoli Paper Abstract Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.]]></summary></entry><entry><title type="html">Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability</title><link href="http://localhost:4000/publications/2505.16789/" rel="alternate" type="text/html" title="Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability" /><published>2025-05-22T00:00:00-04:00</published><updated>2025-05-22T00:00:00-04:00</updated><id>http://localhost:4000/publications/2505.16789</id><content type="html" xml:base="http://localhost:4000/publications/2505.16789/"><![CDATA[<p><em>Punya Syon Pandey, Samuel Simko, Kellin Pelrine, Zhijing Jin</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2505.16789" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Misalignment, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity within our experimental datasets. We then evaluate the adversarial performance of these fine-tuned models and assess how dataset factors correlate with attack success rates. Lastly, we explore potential causal links, offering new insights into adversarial defense strategies and highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_misalignment.</p>]]></content><author><name>Kellin Pelrine</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Punya Syon Pandey, Samuel Simko, Kellin Pelrine, Zhijing Jin Paper Abstract As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Misalignment, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity within our experimental datasets. We then evaluate the adversarial performance of these fine-tuned models and assess how dataset factors correlate with attack success rates. Lastly, we explore potential causal links, offering new insights into adversarial defense strategies and highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_misalignment.]]></summary></entry><entry><title type="html">The Structural Safety Generalization Problem</title><link href="http://localhost:4000/publications/2504.09712/" rel="alternate" type="text/html" title="The Structural Safety Generalization Problem" /><published>2025-04-13T00:00:00-04:00</published><updated>2025-04-13T00:00:00-04:00</updated><id>http://localhost:4000/publications/2504.09712</id><content type="html" xml:base="http://localhost:4000/publications/2504.09712/"><![CDATA[<p><em>Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri, Reihaneh Rabbany, Kellin Pelrine</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2504.09712" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>None</p>]]></content><author><name>Reihaneh Rabbany</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri, Reihaneh Rabbany, Kellin Pelrine Paper Abstract None]]></summary></entry><entry><title type="html">Scaling Trends for Data Poisoning in LLMs</title><link href="http://localhost:4000/publications/10.1609-aaai.v39i26.34929/" rel="alternate" type="text/html" title="Scaling Trends for Data Poisoning in LLMs" /><published>2025-04-11T00:00:00-04:00</published><updated>2025-04-11T00:00:00-04:00</updated><id>http://localhost:4000/publications/10.1609-aaai.v39i26.34929</id><content type="html" xml:base="http://localhost:4000/publications/10.1609-aaai.v39i26.34929/"><![CDATA[<p><em>Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine</em></p>

<p><strong>AAAI Conference on Artificial Intelligence</strong></p>

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://doi.org/10.1609/aaai.v39i26.34929" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>None</p>]]></content><author><name>Kellin Pelrine</name></author><category term="Publications" /><category term="AAAI Conference on Artificial Intelligence" /><summary type="html"><![CDATA[Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine AAAI Conference on Artificial Intelligence Paper Abstract None]]></summary></entry><entry><title type="html">Can Go AIs Be Adversarially Robust?</title><link href="http://localhost:4000/publications/10.1609-aaai.v39i26.34980/" rel="alternate" type="text/html" title="Can Go AIs Be Adversarially Robust?" /><published>2025-04-11T00:00:00-04:00</published><updated>2025-04-11T00:00:00-04:00</updated><id>http://localhost:4000/publications/10.1609-aaai.v39i26.34980</id><content type="html" xml:base="http://localhost:4000/publications/10.1609-aaai.v39i26.34980/"><![CDATA[<p><em>Tom Tseng, Euan McLean, Kellin Pelrine, T. T. Wang, Adam Gleave</em></p>

<p><strong>AAAI Conference on Artificial Intelligence</strong></p>

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://doi.org/10.1609/aaai.v39i26.34980" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>None</p>]]></content><author><name>Kellin Pelrine</name></author><category term="Publications" /><category term="AAAI Conference on Artificial Intelligence" /><summary type="html"><![CDATA[Tom Tseng, Euan McLean, Kellin Pelrine, T. T. Wang, Adam Gleave AAAI Conference on Artificial Intelligence Paper Abstract None]]></summary></entry><entry><title type="html">From Intuition to Understanding: Using AI Peers to Overcome Physics Misconceptions</title><link href="http://localhost:4000/publications/2504.00408/" rel="alternate" type="text/html" title="From Intuition to Understanding: Using AI Peers to Overcome Physics Misconceptions" /><published>2025-04-01T00:00:00-04:00</published><updated>2025-04-01T00:00:00-04:00</updated><id>http://localhost:4000/publications/2504.00408</id><content type="html" xml:base="http://localhost:4000/publications/2504.00408/"><![CDATA[<p><em>Ruben Weijers, Denton Wu, Hannah Betts, Tamara Jacod, Yuxiang Guan, Vidya Sujaya, Kushal Dev, Toshali Goel, William Delooze, Reihaneh Rabbany, Ying Wu, J. Godbout, Kellin Pelrine</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2504.00408" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>None</p>]]></content><author><name>Reihaneh Rabbany</name></author><category term="Publications" /><category term="" /><summary type="html"><![CDATA[Ruben Weijers, Denton Wu, Hannah Betts, Tamara Jacod, Yuxiang Guan, Vidya Sujaya, Kushal Dev, Toshali Goel, William Delooze, Reihaneh Rabbany, Ying Wu, J. Godbout, Kellin Pelrine Paper Abstract None]]></summary></entry></feed>
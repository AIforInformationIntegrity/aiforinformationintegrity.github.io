<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets - AI4</title>
<meta name="description" content="D. Beaini, Shenyang Huang, Joao Alex Cunha, Gabriela Moisescu-Pareja, Oleksandr Dymov, Sam Maddrell-Mander, Callum McLean, Frederik Wenkel, Luis Muller, Jama Hussein Mohamud, Alipanah Parviz, Michael Craig, Michal Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis, M. Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Thérence Bois, Andrew W. Fitzgibbon, Bla.zej Banaszewski, Chad Martin, Dominic Masters  International Conference on Learning Representations                                   Paper                                            Abstract  Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets">
<meta property="og:url" content="http://localhost:4000/publications/2310.04292/">


  <meta property="og:description" content="D. Beaini, Shenyang Huang, Joao Alex Cunha, Gabriela Moisescu-Pareja, Oleksandr Dymov, Sam Maddrell-Mander, Callum McLean, Frederik Wenkel, Luis Muller, Jama Hussein Mohamud, Alipanah Parviz, Michael Craig, Michal Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis, M. Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Thérence Bois, Andrew W. Fitzgibbon, Bla.zej Banaszewski, Chad Martin, Dominic Masters  International Conference on Learning Representations                                   Paper                                            Abstract  Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.">



  <meta property="og:image" content="http://localhost:4000/assets/images/logo/logo.png">





  <meta property="article:published_time" content="2023-10-06T00:00:00-04:00">






<link rel="canonical" href="http://localhost:4000/publications/2310.04292/">












<!-- end _includes/seo.html -->


<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript">
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- Add favicon -->
<link rel="icon" type="image/png" href="/assets/images/logo/favicon.png">

<!-- SEO meta tags -->
<meta name="author" content="D. Beaini, Shenyang Huang, Joao Alex Cunha, Gabriela Moisescu-Pareja, Oleksandr Dymov, Sam Maddrell-Mander, Callum McLean, Frederik Wenkel, Luis Muller, Jama Hussein Mohamud, Alipanah Parviz, Michael Craig, Michal Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis, M. Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Thérence Bois, Andrew W. Fitzgibbon, Bla.zej Banaszewski, Chad Martin, Dominic Masters">
<meta name="description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">

<!-- Open Graph and Twitter Card data -->
<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets">
<meta property="og:description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">



<!-- particles.js -->
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="/assets/js/particles-config.js"></script>
  </head>

  <body class="layout--publication wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!-- Overrides and modified from: https://raw.githubusercontent.com/mmistakes/minimal-mistakes/master/_includes/masthead.html -->



<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-logo" href="/"><img src="/assets/images/logo/logo.png"
            alt="AI4"></a>
        
        <a class="site-title" href="/">
          AI4
          
        </a>
        <ul class="visible-links">

          
          <li class="masthead__menu-item">
            <a href="/mission" >Mission</a>
          </li>
          
          <li class="masthead__menu-item">
            <a href="/" >Research</a>
            <ul class="submenu"><li class="masthead__menu-item">
                <a href="/ideology-and-polarization/" >Ideology & Polarization</a>
              </li><li class="masthead__menu-item">
                <a href="/information-integrity/" >Information Integrity</a>
              </li><li class="masthead__menu-item">
                <a href="/social-simulations/" >Social Simulations</a>
              </li></ul>
          </li>
          

          

          
          <li class="masthead__menu-item">
            <a href="/people/" >People</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/funding/" >Funding</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/publications/" >Publications</a>
          </li>
          </ul>

        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search search-button"></i>
        </button>
        

        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>

        <ul class="hidden-links hidden"></ul>

      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">


    
    <div class="author-avatar-v2">
        <img loading="" src="/assets/images/bio/placeholder/martlet.jpeg" alt="An image of "
            class="u-photo" itemprop="image">
    </div>
    


    <div class="author__content">

        <h3 class="author-name">
            
            
            
        </h3>

        

        

        <div class="author__urls-wrapper">
            <button class="btn btn--inverse">Follow</button>

            <ul class="author__urls">

                

                <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
            </ul>
        </div>

    </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets">
    <meta itemprop="description" content="D. Beaini, Shenyang Huang, Joao Alex Cunha, Gabriela Moisescu-Pareja, Oleksandr Dymov, Sam Maddrell-Mander, Callum McLean, Frederik Wenkel, Luis Muller, Jama Hussein Mohamud, Alipanah Parviz, Michael Craig, Michal Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis, M. Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Thérence Bois, Andrew W. Fitzgibbon, Bla.zej Banaszewski, Chad Martin, Dominic MastersInternational Conference on Learning Representations                            Paper                            AbstractRecently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.">
    <meta itemprop="datePublished" content="2023-10-06T00:00:00-04:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/publications/2310.04292/" itemprop="url">Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "http://localhost:4000"
    }
    ,
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Publications",
      "item": "http://localhost:4000/publications/"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Towards Foundational Models for Molecular Learning on Lar...",
      "item": "http://localhost:4000/publications/2310.04292/"
    }
    
  ]
}
</script>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets",
  "author": [
    
    
    {
      "@type": "Person",
      "name": "D. Beaini"
    },
    
    {
      "@type": "Person",
      "name": "Shenyang Huang"
    },
    
    {
      "@type": "Person",
      "name": "Joao Alex Cunha"
    },
    
    {
      "@type": "Person",
      "name": "Gabriela Moisescu-Pareja"
    },
    
    {
      "@type": "Person",
      "name": "Oleksandr Dymov"
    },
    
    {
      "@type": "Person",
      "name": "Sam Maddrell-Mander"
    },
    
    {
      "@type": "Person",
      "name": "Callum McLean"
    },
    
    {
      "@type": "Person",
      "name": "Frederik Wenkel"
    },
    
    {
      "@type": "Person",
      "name": "Luis Muller"
    },
    
    {
      "@type": "Person",
      "name": "Jama Hussein Mohamud"
    },
    
    {
      "@type": "Person",
      "name": "Alipanah Parviz"
    },
    
    {
      "@type": "Person",
      "name": "Michael Craig"
    },
    
    {
      "@type": "Person",
      "name": "Michal Koziarski"
    },
    
    {
      "@type": "Person",
      "name": "Jiarui Lu"
    },
    
    {
      "@type": "Person",
      "name": "Zhaocheng Zhu"
    },
    
    {
      "@type": "Person",
      "name": "Cristian Gabellini"
    },
    
    {
      "@type": "Person",
      "name": "Kerstin Klaser"
    },
    
    {
      "@type": "Person",
      "name": "Josef Dean"
    },
    
    {
      "@type": "Person",
      "name": "Cas Wognum"
    },
    
    {
      "@type": "Person",
      "name": "Maciej Sypetkowski"
    },
    
    {
      "@type": "Person",
      "name": "Guillaume Rabusseau"
    },
    
    {
      "@type": "Person",
      "name": "Reihaneh Rabbany"
    },
    
    {
      "@type": "Person",
      "name": "Jian Tang"
    },
    
    {
      "@type": "Person",
      "name": "Christopher Morris"
    },
    
    {
      "@type": "Person",
      "name": "Ioannis Koutis"
    },
    
    {
      "@type": "Person",
      "name": "M. Ravanelli"
    },
    
    {
      "@type": "Person",
      "name": "Guy Wolf"
    },
    
    {
      "@type": "Person",
      "name": "Prudencio Tossou"
    },
    
    {
      "@type": "Person",
      "name": "Hadrien Mary"
    },
    
    {
      "@type": "Person",
      "name": "Thérence Bois"
    },
    
    {
      "@type": "Person",
      "name": "Andrew W. Fitzgibbon"
    },
    
    {
      "@type": "Person",
      "name": "Bla.zej Banaszewski"
    },
    
    {
      "@type": "Person",
      "name": "Chad Martin"
    },
    
    {
      "@type": "Person",
      "name": "Dominic Masters"
    }
    
  ],
  "datePublished": "",
  "publisher": "International Conference on Learning Representations",
  
  
  
  "keywords": ["International Conference on Learning Representations"],
  
  "isAccessibleForFree": false
}
</script>

<p><em>D. Beaini, Shenyang Huang, Joao Alex Cunha, Gabriela Moisescu-Pareja, Oleksandr Dymov, Sam Maddrell-Mander, Callum McLean, Frederik Wenkel, Luis Muller, Jama Hussein Mohamud, Alipanah Parviz, Michael Craig, Michal Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis, M. Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Thérence Bois, Andrew W. Fitzgibbon, Bla.zej Banaszewski, Chad Martin, Dominic Masters</em></p>

<p><strong>International Conference on Learning Representations</strong></p>

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2310.04292" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.</p>


<div>
  







</div>
        <div><a href="https://arxiv.org/abs/2310.04292" class="btn btn--primary">Direct Link</a></div>
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#international-conference-on-learning-representations" class="page__taxonomy-item p-category" rel="tag">International Conference on Learning Representations</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#publications" class="page__taxonomy-item p-category" rel="tag">Publications</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-10-06T00:00:00-04:00">October 6, 2023</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=Towards+Foundational+Models+for+Molecular+Learning+on+Large-Scale+Multi-Task+Datasets%20http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2310.04292%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2310.04292%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/publications/2310.04292/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/publications/ideology-and-polarization/2308.13699/" class="pagination--pager" title="Party Prediction for Twitter
">Previous</a>
    
    
      <a href="/publications/online-toxicity/2310.18330/" class="pagination--pager" title="Towards Detecting Contextual Real-Time Toxicity for In-Game Chat
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
    <ul class="social-icons">
        

        
        
        
        <li><a href="https://github.com/AIforInformationIntegrity" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
        
        
        <li><a href="https://x.com" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
        
        

        
    </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a
        href="http://localhost:4000">AI4</a>.</div>
      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>

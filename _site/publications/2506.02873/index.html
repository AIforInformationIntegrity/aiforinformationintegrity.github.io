<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics - AI4</title>
<meta name="description" content="Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine                                     Paper                                            Abstract  Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders’’ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model’s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval">


  <meta name="author" content="Kellin Pelrine">
  
  <meta property="article:author" content="Kellin Pelrine">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics">
<meta property="og:url" content="http://localhost:4000/publications/2506.02873/">


  <meta property="og:description" content="Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine                                     Paper                                            Abstract  Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders’’ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model’s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval">



  <meta property="og:image" content="http://localhost:4000/assets/images/logo/logo.png">





  <meta property="article:published_time" content="2025-06-03T00:00:00-04:00">






<link rel="canonical" href="http://localhost:4000/publications/2506.02873/">












<!-- end _includes/seo.html -->


<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript">
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- Add favicon -->
<link rel="icon" type="image/png" href="/assets/images/logo/favicon.png">

<!-- SEO meta tags -->
<meta name="author" content="Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine">
<meta name="description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">

<!-- Open Graph and Twitter Card data -->
<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics">
<meta property="og:description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">

<meta property="og:url" content="">


<!-- particles.js -->
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="/assets/js/particles-config.js"></script>
  </head>

  <body class="layout--publication wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!-- Overrides and modified from: https://raw.githubusercontent.com/mmistakes/minimal-mistakes/master/_includes/masthead.html -->



<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-logo" href="/"><img src="/assets/images/logo/logo.png"
            alt="AI4"></a>
        
        <a class="site-title" href="/">
          AI4
          
        </a>
        <ul class="visible-links">

          
          <li class="masthead__menu-item">
            <a href="/mission" >Mission</a>
          </li>
          
          <li class="masthead__menu-item">
            <a href="/" >Research</a>
            <ul class="submenu"><li class="masthead__menu-item">
                <a href="/ideology-and-polarization/" >Ideology & Polarization</a>
              </li><li class="masthead__menu-item">
                <a href="/information-integrity/" >Information Integrity</a>
              </li><li class="masthead__menu-item">
                <a href="/social-simulations/" >Social Simulations</a>
              </li></ul>
          </li>
          

          

          
          <li class="masthead__menu-item">
            <a href="/people/" >People</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/funding/" >Funding</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/publications/" >Publications</a>
          </li>
          </ul>

        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search search-button"></i>
        </button>
        

        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>

        <ul class="hidden-links hidden"></ul>

      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">


    
    <div class="author-avatar-v2">
        <img loading="" src="/assets/images/bio/kellin.pelrine.avatar.webp" alt="An image of Kellin Pelrine"
            class="u-photo" itemprop="image">
    </div>

    


    <div class="author__content">

        <h3 class="author-name">
            
            <a href="https://kellinpelrine.github.io/"
                aria-label="Access the primary external link of this author">Kellin Pelrine</a>
            
        </h3>

        
        <div class="author-bio">
            <p>AI Security, AI Agents. Cross-functional solutions on a foundation of technical research.</p>

        </div>
        

        
        <div class="author-note">
            <p>Research Scientist @ FAR.AI</p>

        </div>
        

        <div class="author__urls-wrapper">
            <button class="btn btn--inverse">Follow</button>

            <ul class="author__urls">

                

                
                


                
                <li>
                    <a href="https://kellinpelrine.github.io/" class="author-button">
                        <i class="fas fa-link author-button" aria-label="Access author's external link for fas fa-link">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://github.com/kellinpelrine/" class="author-button">
                        <i class="fab fa-github author-button" aria-label="Access author's external link for fab fa-github">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://scholar.google.com/citations?hl=en&user=_s2HT_0AAAAJ" class="author-button">
                        <i class="fas fa-graduation-cap author-button" aria-label="Access author's external link for fas fa-graduation-cap">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://www.linkedin.com/in/kellin-pelrine/" class="author-button">
                        <i class="fab fa-linkedin author-button" aria-label="Access author's external link for fab fa-linkedin">
                        </i>
                    </a>
                </li>

                
                
                

                <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
            </ul>
        </div>

    </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics">
    <meta itemprop="description" content="Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine                            Paper                            AbstractPersuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders’’ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model’s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval">
    <meta itemprop="datePublished" content="2025-06-03T00:00:00-04:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/publications/2506.02873/" itemprop="url">It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "http://localhost:4000"
    }
    ,
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Publications",
      "item": "http://localhost:4000/publications/"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "It's the Thought that Counts: Evaluating the Attempts of ...",
      "item": "http://localhost:4000/publications/2506.02873/"
    }
    
  ]
}
</script>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
  "author": [
    
    
    {
      "@type": "Person",
      "name": "Matthew Kowal"
    },
    
    {
      "@type": "Person",
      "name": "Jasper Timm"
    },
    
    {
      "@type": "Person",
      "name": "J. Godbout"
    },
    
    {
      "@type": "Person",
      "name": "Thomas H Costello"
    },
    
    {
      "@type": "Person",
      "name": "A. Arechar"
    },
    
    {
      "@type": "Person",
      "name": "Gordon Pennycook"
    },
    
    {
      "@type": "Person",
      "name": "David Rand"
    },
    
    {
      "@type": "Person",
      "name": "Adam Gleave"
    },
    
    {
      "@type": "Person",
      "name": "Kellin Pelrine"
    }
    
  ],
  "datePublished": "",
  "publisher": "",
  
  "url": "",
  
  
  
  "keywords": [""],
  
  "isAccessibleForFree": false
}
</script>

<p><em>Matthew Kowal, Jasper Timm, J. Godbout, Thomas H Costello, A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2506.02873" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders’’ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model’s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval</p>


<div>
  







</div>
        <div><a href="https://arxiv.org/abs/2506.02873" class="btn btn--primary">Direct Link</a></div>
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/" class="page__taxonomy-item p-category" rel="tag"></a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#publications" class="page__taxonomy-item p-category" rel="tag">Publications</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-06-03T00:00:00-04:00">June 3, 2025</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=It%27s+the+Thought+that+Counts%3A+Evaluating+the+Attempts+of+Frontier+LLMs+to+Persuade+on+Harmful+Topics%20http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2506.02873%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2506.02873%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/publications/2506.02873/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/publications/2506.02451/" class="pagination--pager" title="Weak Supervision for Real World Graphs
">Previous</a>
    
    
      <a href="/publications/2506.05393/" class="pagination--pager" title="Are Large Language Models Good Temporal Graph Learners?
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
    <ul class="social-icons">
        

        
        
        
        <li><a href="https://github.com/AIforInformationIntegrity" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
        
        
        <li><a href="https://x.com" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
        
        

        
    </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a
        href="http://localhost:4000">AI4</a>.</div>
      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>

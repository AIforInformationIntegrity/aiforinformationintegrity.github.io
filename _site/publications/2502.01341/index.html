<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding - AI4</title>
<meta name="description" content="Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-Andr’e Noel, Sathwik Tejaswi Madhusudhan, M. Pedersoli, Bang Liu, Nicolas Chapados, Y. Bengio, Enamul Hoque, Christopher Pal, I. Laradji, David Vázquez, Perouz Taslakian, Spandana Gella, Sai Rajeswar                                     Paper                                            Abstract  Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.">


  <meta name="author" content="Aarash Feizi">
  
  <meta property="article:author" content="Aarash Feizi">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding">
<meta property="og:url" content="http://localhost:4000/publications/2502.01341/">


  <meta property="og:description" content="Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-Andr’e Noel, Sathwik Tejaswi Madhusudhan, M. Pedersoli, Bang Liu, Nicolas Chapados, Y. Bengio, Enamul Hoque, Christopher Pal, I. Laradji, David Vázquez, Perouz Taslakian, Spandana Gella, Sai Rajeswar                                     Paper                                            Abstract  Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.">



  <meta property="og:image" content="http://localhost:4000/assets/images/logo/logo.png">





  <meta property="article:published_time" content="2025-02-03T00:00:00-05:00">






<link rel="canonical" href="http://localhost:4000/publications/2502.01341/">












<!-- end _includes/seo.html -->


<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript">
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- Add favicon -->
<link rel="icon" type="image/png" href="/assets/images/logo/favicon.png">

<!-- SEO meta tags -->
<meta name="author" content="Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-Andr'e Noel, Sathwik Tejaswi Madhusudhan, M. Pedersoli, Bang Liu, Nicolas Chapados, Y. Bengio, Enamul Hoque, Christopher Pal, I. Laradji, David Vázquez, Perouz Taslakian, Spandana Gella, Sai Rajeswar">
<meta name="description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">

<!-- Open Graph and Twitter Card data -->
<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding">
<meta property="og:description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">



<!-- particles.js -->
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="/assets/js/particles-config.js"></script>
  </head>

  <body class="layout--publication wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!-- Overrides and modified from: https://raw.githubusercontent.com/mmistakes/minimal-mistakes/master/_includes/masthead.html -->



<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-logo" href="/"><img src="/assets/images/logo/logo.png"
            alt="AI4"></a>
        
        <a class="site-title" href="/">
          AI4
          
        </a>
        <ul class="visible-links">

          
          <li class="masthead__menu-item">
            <a href="/mission" >Mission</a>
          </li>
          
          <li class="masthead__menu-item">
            <a href="/" >Research</a>
            <ul class="submenu"><li class="masthead__menu-item">
                <a href="/ideology-and-polarization/" >Ideology & Polarization</a>
              </li><li class="masthead__menu-item">
                <a href="/information-integrity/" >Information Integrity</a>
              </li><li class="masthead__menu-item">
                <a href="/social-simulations/" >Social Simulations</a>
              </li></ul>
          </li>
          

          

          
          <li class="masthead__menu-item">
            <a href="/people/" >People</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/funding/" >Funding</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/publications/" >Publications</a>
          </li>
          </ul>

        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search search-button"></i>
        </button>
        

        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>

        <ul class="hidden-links hidden"></ul>

      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">


    
    <div class="author-avatar-v2">
        <img loading="" src="/assets/images/bio/aarash.feizi.avatar.webp" alt="An image of Aarash Feizi"
            class="u-photo" itemprop="image">
    </div>

    


    <div class="author__content">

        <h3 class="author-name">
            
            <a href="https://github.com/aarashfeizi"
                aria-label="Access the primary external link of this author">Aarash Feizi</a>
            
        </h3>

        
        <div class="author-bio">
            <p>Works on multi-modal self-supervised learning and applied machine learning techniques in CV and NLP for social good with real-world impacts.</p>

        </div>
        

        

        <div class="author__urls-wrapper">
            <button class="btn btn--inverse">Follow</button>

            <ul class="author__urls">

                

                
                


                
                <li>
                    <a href="https://github.com/aarashfeizi" class="author-button">
                        <i class="fab fa-github author-button" aria-label="Access author's external link for fab fa-github">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://scholar.google.com/citations?user=wdZdCMoAAAAJ&hl=en" class="author-button">
                        <i class="fas fa-graduation-cap author-button" aria-label="Access author's external link for fas fa-graduation-cap">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://www.linkedin.com/in/aarashfeizi/" class="author-button">
                        <i class="fab fa-linkedin author-button" aria-label="Access author's external link for fab fa-linkedin">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://aarashfeizi.github.io/" class="author-button">
                        <i class="fas fa-link author-button" aria-label="Access author's external link for fas fa-link">
                        </i>
                    </a>
                </li>

                
                
                

                <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
            </ul>
        </div>

    </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding">
    <meta itemprop="description" content="Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-Andr’e Noel, Sathwik Tejaswi Madhusudhan, M. Pedersoli, Bang Liu, Nicolas Chapados, Y. Bengio, Enamul Hoque, Christopher Pal, I. Laradji, David Vázquez, Perouz Taslakian, Spandana Gella, Sai Rajeswar                            Paper                            AbstractAligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.">
    <meta itemprop="datePublished" content="2025-02-03T00:00:00-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/publications/2502.01341/" itemprop="url">AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "http://localhost:4000"
    }
    ,
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Publications",
      "item": "http://localhost:4000/publications/"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "AlignVLM: Bridging Vision and Language Latent Spaces for ...",
      "item": "http://localhost:4000/publications/2502.01341/"
    }
    
  ]
}
</script>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding",
  "author": [
    
    
    {
      "@type": "Person",
      "name": "Ahmed Masry"
    },
    
    {
      "@type": "Person",
      "name": "Juan A. Rodriguez"
    },
    
    {
      "@type": "Person",
      "name": "Tianyu Zhang"
    },
    
    {
      "@type": "Person",
      "name": "Suyuchen Wang"
    },
    
    {
      "@type": "Person",
      "name": "Chao Wang"
    },
    
    {
      "@type": "Person",
      "name": "Aarash Feizi"
    },
    
    {
      "@type": "Person",
      "name": "Akshay Kalkunte Suresh"
    },
    
    {
      "@type": "Person",
      "name": "Abhay Puri"
    },
    
    {
      "@type": "Person",
      "name": "Xiangru Jian"
    },
    
    {
      "@type": "Person",
      "name": "Pierre-Andr'e Noel"
    },
    
    {
      "@type": "Person",
      "name": "Sathwik Tejaswi Madhusudhan"
    },
    
    {
      "@type": "Person",
      "name": "M. Pedersoli"
    },
    
    {
      "@type": "Person",
      "name": "Bang Liu"
    },
    
    {
      "@type": "Person",
      "name": "Nicolas Chapados"
    },
    
    {
      "@type": "Person",
      "name": "Y. Bengio"
    },
    
    {
      "@type": "Person",
      "name": "Enamul Hoque"
    },
    
    {
      "@type": "Person",
      "name": "Christopher Pal"
    },
    
    {
      "@type": "Person",
      "name": "I. Laradji"
    },
    
    {
      "@type": "Person",
      "name": "David Vázquez"
    },
    
    {
      "@type": "Person",
      "name": "Perouz Taslakian"
    },
    
    {
      "@type": "Person",
      "name": "Spandana Gella"
    },
    
    {
      "@type": "Person",
      "name": "Sai Rajeswar"
    }
    
  ],
  "datePublished": "",
  "publisher": "",
  
  
  
  "keywords": [""],
  
  "isAccessibleForFree": false
}
</script>

<p><em>Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-Andr’e Noel, Sathwik Tejaswi Madhusudhan, M. Pedersoli, Bang Liu, Nicolas Chapados, Y. Bengio, Enamul Hoque, Christopher Pal, I. Laradji, David Vázquez, Perouz Taslakian, Spandana Gella, Sai Rajeswar</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2502.01341" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.</p>


<div>
  







</div>
        <div><a href="https://arxiv.org/abs/2502.01341" class="btn btn--primary">Direct Link</a></div>
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/" class="page__taxonomy-item p-category" rel="tag"></a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#publications" class="page__taxonomy-item p-category" rel="tag">Publications</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-02-03T00:00:00-05:00">February 3, 2025</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=AlignVLM%3A+Bridging+Vision+and+Language+Latent+Spaces+for+Multimodal+Understanding%20http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2502.01341%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2502.01341%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/publications/2502.01341/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/teaching/comp551-W25/" class="pagination--pager" title="Applied Machine Learning
">Previous</a>
    
    
      <a href="/publications/2502.15210/" class="pagination--pager" title="PairBench: Are Vision-Language Models Reliable at Comparing What They See?
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
    <ul class="social-icons">
        

        
        
        
        <li><a href="https://github.com/AIforInformationIntegrity" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
        
        
        <li><a href="https://x.com" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
        
        

        
    </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a
        href="http://localhost:4000">AI4</a>.</div>
      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>

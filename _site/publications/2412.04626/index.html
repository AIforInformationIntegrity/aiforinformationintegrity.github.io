<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks - AI4</title>
<meta name="description" content="Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, Franccois Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-Andre Noel, M. L. Richter, Saverio Vadacchino, Shubbam Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Noah Bolger, Kurt MacDonald, Simon Fauvel, Sathwik Tejaswi, Srinivas Sunkara, João Monteiro, K. Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh Kharagani, Sean Hughes, M. Ozsu, Siva Reddy, M. Pedersoli, Y. Bengio, Christopher Pal, I. Laradji, Spandanna Gella, Perouz Taslakian, David Vázquez, Sai Rajeswar                                     Paper                                            Abstract  Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .">


  <meta name="author" content="Aarash Feizi">
  
  <meta property="article:author" content="Aarash Feizi">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks">
<meta property="og:url" content="http://localhost:4000/publications/2412.04626/">


  <meta property="og:description" content="Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, Franccois Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-Andre Noel, M. L. Richter, Saverio Vadacchino, Shubbam Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Noah Bolger, Kurt MacDonald, Simon Fauvel, Sathwik Tejaswi, Srinivas Sunkara, João Monteiro, K. Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh Kharagani, Sean Hughes, M. Ozsu, Siva Reddy, M. Pedersoli, Y. Bengio, Christopher Pal, I. Laradji, Spandanna Gella, Perouz Taslakian, David Vázquez, Sai Rajeswar                                     Paper                                            Abstract  Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .">



  <meta property="og:image" content="http://localhost:4000/assets/images/logo/logo.png">





  <meta property="article:published_time" content="2024-12-05T00:00:00-05:00">






<link rel="canonical" href="http://localhost:4000/publications/2412.04626/">












<!-- end _includes/seo.html -->


<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript">
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- Add favicon -->
<link rel="icon" type="image/png" href="/assets/images/logo/favicon.png">

<!-- SEO meta tags -->
<meta name="author" content="Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, Franccois Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-Andre Noel, M. L. Richter, Saverio Vadacchino, Shubbam Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Noah Bolger, Kurt MacDonald, Simon Fauvel, Sathwik Tejaswi, Srinivas Sunkara, João Monteiro, K. Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh Kharagani, Sean Hughes, M. Ozsu, Siva Reddy, M. Pedersoli, Y. Bengio, Christopher Pal, I. Laradji, Spandanna Gella, Perouz Taslakian, David Vázquez, Sai Rajeswar">
<meta name="description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">

<!-- Open Graph and Twitter Card data -->
<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI4">
<meta property="og:title" content="BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks">
<meta property="og:description"
    content="AI Institute for Information Integrity (AI4) is a research institute focused on developing AI technologies that enhance information integrity.">



<!-- particles.js -->
<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
<script src="/assets/js/particles-config.js"></script>
  </head>

  <body class="layout--publication wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!-- Overrides and modified from: https://raw.githubusercontent.com/mmistakes/minimal-mistakes/master/_includes/masthead.html -->



<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-logo" href="/"><img src="/assets/images/logo/logo.png"
            alt="AI4"></a>
        
        <a class="site-title" href="/">
          AI4
          
        </a>
        <ul class="visible-links">

          
          <li class="masthead__menu-item">
            <a href="/mission" >Mission</a>
          </li>
          
          <li class="masthead__menu-item">
            <a href="/" >Research</a>
            <ul class="submenu"><li class="masthead__menu-item">
                <a href="/ideology-and-polarization/" >Ideology & Polarization</a>
              </li><li class="masthead__menu-item">
                <a href="/information-integrity/" >Information Integrity</a>
              </li><li class="masthead__menu-item">
                <a href="/social-simulations/" >Social Simulations</a>
              </li></ul>
          </li>
          

          

          
          <li class="masthead__menu-item">
            <a href="/people/" >People</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/funding/" >Funding</a>
          </li>
          

          
          <li class="masthead__menu-item">
            <a href="/publications/" >Publications</a>
          </li>
          </ul>

        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search search-button"></i>
        </button>
        

        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>

        <ul class="hidden-links hidden"></ul>

      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">


    
    <div class="author-avatar-v2">
        <img loading="" src="/assets/images/bio/aarash.feizi.avatar.webp" alt="An image of Aarash Feizi"
            class="u-photo" itemprop="image">
    </div>

    


    <div class="author__content">

        <h3 class="author-name">
            
            <a href="https://github.com/aarashfeizi"
                aria-label="Access the primary external link of this author">Aarash Feizi</a>
            
        </h3>

        
        <div class="author-bio">
            <p>Works on multi-modal self-supervised learning and applied machine learning techniques in CV and NLP for social good with real-world impacts.</p>

        </div>
        

        

        <div class="author__urls-wrapper">
            <button class="btn btn--inverse">Follow</button>

            <ul class="author__urls">

                

                
                


                
                <li>
                    <a href="https://github.com/aarashfeizi" class="author-button">
                        <i class="fab fa-github author-button" aria-label="Access author's external link for fab fa-github">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://scholar.google.com/citations?user=wdZdCMoAAAAJ&hl=en" class="author-button">
                        <i class="fas fa-graduation-cap author-button" aria-label="Access author's external link for fas fa-graduation-cap">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://www.linkedin.com/in/aarashfeizi/" class="author-button">
                        <i class="fab fa-linkedin author-button" aria-label="Access author's external link for fab fa-linkedin">
                        </i>
                    </a>
                </li>

                
                
                


                
                <li>
                    <a href="https://aarashfeizi.github.io/" class="author-button">
                        <i class="fas fa-link author-button" aria-label="Access author's external link for fas fa-link">
                        </i>
                    </a>
                </li>

                
                
                

                <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
            </ul>
        </div>

    </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks">
    <meta itemprop="description" content="Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, Franccois Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-Andre Noel, M. L. Richter, Saverio Vadacchino, Shubbam Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Noah Bolger, Kurt MacDonald, Simon Fauvel, Sathwik Tejaswi, Srinivas Sunkara, João Monteiro, K. Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh Kharagani, Sean Hughes, M. Ozsu, Siva Reddy, M. Pedersoli, Y. Bengio, Christopher Pal, I. Laradji, Spandanna Gella, Perouz Taslakian, David Vázquez, Sai Rajeswar                            Paper                            AbstractMultimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .">
    <meta itemprop="datePublished" content="2024-12-05T00:00:00-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/publications/2412.04626/" itemprop="url">BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "http://localhost:4000"
    }
    ,
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Publications",
      "item": "http://localhost:4000/publications/"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "BigDocs: An Open and Permissively-Licensed Dataset for Tr...",
      "item": "http://localhost:4000/publications/2412.04626/"
    }
    
  ]
}
</script>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks",
  "author": [
    
    
    {
      "@type": "Person",
      "name": "Juan Rodriguez"
    },
    
    {
      "@type": "Person",
      "name": "Xiangru Jian"
    },
    
    {
      "@type": "Person",
      "name": "Siba Smarak Panigrahi"
    },
    
    {
      "@type": "Person",
      "name": "Tianyu Zhang"
    },
    
    {
      "@type": "Person",
      "name": "Aarash Feizi"
    },
    
    {
      "@type": "Person",
      "name": "Abhay Puri"
    },
    
    {
      "@type": "Person",
      "name": "Akshay Kalkunte"
    },
    
    {
      "@type": "Person",
      "name": "Franccois Savard"
    },
    
    {
      "@type": "Person",
      "name": "Ahmed Masry"
    },
    
    {
      "@type": "Person",
      "name": "Shravan Nayak"
    },
    
    {
      "@type": "Person",
      "name": "Rabiul Awal"
    },
    
    {
      "@type": "Person",
      "name": "Mahsa Massoud"
    },
    
    {
      "@type": "Person",
      "name": "Amirhossein Abaskohi"
    },
    
    {
      "@type": "Person",
      "name": "Zichao Li"
    },
    
    {
      "@type": "Person",
      "name": "Suyuchen Wang"
    },
    
    {
      "@type": "Person",
      "name": "Pierre-Andre Noel"
    },
    
    {
      "@type": "Person",
      "name": "M. L. Richter"
    },
    
    {
      "@type": "Person",
      "name": "Saverio Vadacchino"
    },
    
    {
      "@type": "Person",
      "name": "Shubbam Agarwal"
    },
    
    {
      "@type": "Person",
      "name": "Sanket Biswas"
    },
    
    {
      "@type": "Person",
      "name": "Sara Shanian"
    },
    
    {
      "@type": "Person",
      "name": "Ying Zhang"
    },
    
    {
      "@type": "Person",
      "name": "Noah Bolger"
    },
    
    {
      "@type": "Person",
      "name": "Kurt MacDonald"
    },
    
    {
      "@type": "Person",
      "name": "Simon Fauvel"
    },
    
    {
      "@type": "Person",
      "name": "Sathwik Tejaswi"
    },
    
    {
      "@type": "Person",
      "name": "Srinivas Sunkara"
    },
    
    {
      "@type": "Person",
      "name": "João Monteiro"
    },
    
    {
      "@type": "Person",
      "name": "K. Dvijotham"
    },
    
    {
      "@type": "Person",
      "name": "Torsten Scholak"
    },
    
    {
      "@type": "Person",
      "name": "Nicolas Chapados"
    },
    
    {
      "@type": "Person",
      "name": "Sepideh Kharagani"
    },
    
    {
      "@type": "Person",
      "name": "Sean Hughes"
    },
    
    {
      "@type": "Person",
      "name": "M. Ozsu"
    },
    
    {
      "@type": "Person",
      "name": "Siva Reddy"
    },
    
    {
      "@type": "Person",
      "name": "M. Pedersoli"
    },
    
    {
      "@type": "Person",
      "name": "Y. Bengio"
    },
    
    {
      "@type": "Person",
      "name": "Christopher Pal"
    },
    
    {
      "@type": "Person",
      "name": "I. Laradji"
    },
    
    {
      "@type": "Person",
      "name": "Spandanna Gella"
    },
    
    {
      "@type": "Person",
      "name": "Perouz Taslakian"
    },
    
    {
      "@type": "Person",
      "name": "David Vázquez"
    },
    
    {
      "@type": "Person",
      "name": "Sai Rajeswar"
    }
    
  ],
  "datePublished": "",
  "publisher": "",
  
  
  
  "keywords": [""],
  
  "isAccessibleForFree": false
}
</script>

<p><em>Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, Franccois Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-Andre Noel, M. L. Richter, Saverio Vadacchino, Shubbam Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Noah Bolger, Kurt MacDonald, Simon Fauvel, Sathwik Tejaswi, Srinivas Sunkara, João Monteiro, K. Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh Kharagani, Sean Hughes, M. Ozsu, Siva Reddy, M. Pedersoli, Y. Bengio, Christopher Pal, I. Laradji, Spandanna Gella, Perouz Taslakian, David Vázquez, Sai Rajeswar</em></p>

<hr />

<div class="publication-links">
    
    
    <a class="publication-ext-link" href="https://arxiv.org/abs/2412.04626" target="_blank">
        <i class="fas fa-book"></i>
        <span>Paper</span>
    </a>
    

    

    

    

    

    

</div>

<h2 id="abstract">Abstract</h2>

<p>Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .</p>


<div>
  







</div>
        <div><a href="https://arxiv.org/abs/2412.04626" class="btn btn--primary">Direct Link</a></div>
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/" class="page__taxonomy-item p-category" rel="tag"></a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#publications" class="page__taxonomy-item p-category" rel="tag">Publications</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-12-05T00:00:00-05:00">December 5, 2024</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=BigDocs%3A+An+Open+and+Permissively-Licensed+Dataset+for+Training+Multimodal+Models+on+Document+and+Code+Tasks%20http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2412.04626%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fpublications%2F2412.04626%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/publications/2412.04626/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/publications/2411.06528/" class="pagination--pager" title="Epistemic Integrity in Large Language Models
">Previous</a>
    
    
      <a href="/publications/10.48550-arXiv.2501.10387/" class="pagination--pager" title="Online Influence Campaigns: Strategies and Vulnerabilities
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
    <ul class="social-icons">
        

        
        
        
        <li><a href="https://github.com/AIforInformationIntegrity" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
        
        
        <li><a href="https://x.com" rel="nofollow noopener noreferrer"><i
                    class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
        
        

        
    </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a
        href="http://localhost:4000">AI4</a>.</div>
      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
